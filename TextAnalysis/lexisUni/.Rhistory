library(data.table)
library(tidyverse)
library(purrr)
### Read in
Windbelt_Project_Database_1_ <- read_csv("G:/Data/wind project dataset/WindbeltProjectDatabase.csv")
wind_df <- Windbelt_Project_Database_1_
directory <- "G:/TextAnalysis/lexisUni/sample_pdfs"
pdfs_2 <- paste(directory, "/", list.files(directory, pattern = "*.pdf", ignore.case = T), sep = "")
pdfs_names <- list.files(directory, pattern = "*.pdf", ignore.case = T)
pdfs_names
pdfs_text <- map(pdfs_2, pdftools::pdf_text)
head(pdfs_text,2)
### Packages
# install.packages("pdftools", "devtools")
library(pdftools)
library(tm)
# library(tm.plugin.lexisnexis)
library(devtools)
# install.packages("tidytext")
library(tidytext)
library(broom)
# install.packages("tidyverse")
library(data.table)
library(tidyverse)
library(purrr)
### Read in
Windbelt_Project_Database_1_ <- read_csv("G:/Data/wind project dataset/WindbeltProjectDatabase.csv")
wind_df <- Windbelt_Project_Database_1_
#each row is a pdf doc name (document) with the full pdf text
my_data <- data_frame(document = pdfs_names, text = pdfs_text)
View(my_data)
# text column : each row is an element of a list
library(stringr)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
p <- purr::as_mapper(~identical(., character(0)))
x[p(x)] <- NA
return(x)
}
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
#each row is a pdf doc name (document) with the full pdf text
my_data <- data_frame(document = pdfs_names, text = pdfs_text)
View(my_data)
# text column : each row is an element of a list
library(stringr)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
p <- purr::as_mapper(~identical(., character(0)))
x[p(x)] <- NA
return(x)
}
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
# dataset with each page in one row
my_data1 <- my_data %>%
unnest() # splits pdf text by page and removes list format ( c("")) since each element of the list is now its own row.
View(my_data1)
library(stringr)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
p <- purr::as_mapper(~identical(., character(0)))
x[p(x)] <- NA
return(x)
}
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
p <- purrr::as_mapper(~identical(., character(0)))
x[p(x)] <- NA
return(x)
}
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
?unnest_tokens()
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = sentences, input = text, token =
"sentences", to_lower = T
# strip_numeric = TRUE
)%>%
filter(!word %in% c("lexis",
"nexis", "Uni",
"about lexisnexis",
"Privacy Policy",
"Terms & Conditions", "Copyright Â© 2018 LexisNexis",
" | ",  "@", "lexisnexis"))
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = sentences, input = text, token =
"sentences", to_lower = T
# strip_numeric = TRUE
)
knitr::opts_chunk$set(echo = TRUE)
View(my_data4)
knitr::opts_chunk$set(echo = TRUE)
my_data4_bind <-my_data4 %>%
left_join(get_sentiments("afinn"), by = "word")
knitr::opts_chunk$set(echo = TRUE)
my_data4_bind <-my_data4 %>%
left_join(get_sentiments("afinn"), by = "sentence")
knitr::opts_chunk$set(echo = TRUE)
my_data4_bind <-my_data4 %>%
left_join(get_sentiments("afinn"), by = "sentences")
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = sentences, input = text, token =
"ngrams", to_lower = T
# strip_numeric = TRUE
)
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = sentences, input = text, token =
"ngrams", n= "5" to_lower = T
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = sentences, input = text, token =
"ngrams", n= "5", to_lower = T
# strip_numeric = TRUE
)
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = sentences, input = text, token = "ngrams", n = "2", to_lower = T
# strip_numeric = TRUE
)
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = sentences, input = text, token = "ngrams", n = 5, to_lower = T
# strip_numeric = TRUE
)
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T
# strip_numeric = TRUE
)
knitr::opts_chunk$set(echo = TRUE)
my_data4_bind <-my_data4 %>%
left_join(get_sentiments("afinn"), by = "ngrams")
my_data4_bind <-my_data4 %>%
left_join(get_sentiments("afinn"), by = "ngrams")
knitr::opts_chunk$set(echo = TRUE)
names(my_data4)
knitr::opts_chunk$set(echo = TRUE)
my_data4_bind <-my_data4 %>%
left_join(get_sentiments("afinn"), by = "word")
knitr::opts_chunk$set(echo = TRUE)
?left_join()
my_data4_bind <-my_data4 %>%
left_join( get_sentiments("afinn"), by = "word")
install.packages("doParallel")
install.packages("foreach")
install.packages("foreign")
install.packages("gplots")
install.packages("Hmisc")
install.packages("iptools")
install.packages("labdsv")
install.packages("leaflet")
install.packages("maptools")
install.packages("PBSmapping")
install.packages("png")
install.packages("rgdal")
install.packages("rgeos")
install.packages("rhandsontable")
install.packages("rjson")
install.packages("shiny")
install.packages("shinyBS")
install.packages("sp")
install.packages("sqldf")
install.packages("vegan")
install.packages("xtable")
library(marxanui)       # Load the R package
launch_app("import")    # Launch the import app to import your own data
library(marxanui)       # Load the R package
source('~/.active-rstudio-document', echo=TRUE)
library(marxanui)       # Load the R package
library(marxanui)       # Load the R package
install.packages("devtools")
library(devtools)
install_github("mattwatts/marxanui")
install.packages("devtools")
library(marxanui)       # Load the R package
install_github("mattwatts/marxanui")
library(devtools)
install_github("mattwatts/marxanui")
library(marxanui)       # Load the R package
launch_app("import")    # Launch the import app to import your own data
launch_app("marxan")    # Launch the marxan app to run Marxan
launch_app("marxan")    # Launch the marxan app to run Marxan
knitr::opts_chunk$set(echo = TRUE)
my_data4_bind <-my_data4 %>%
left_join(get_sentiments("afinn"))
knitr::opts_chunk$set(echo = TRUE)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost.|^stop.'))
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
# install.packages("pdftools", "devtools")
library(pdftools)
library(tm)
# library(tm.plugin.lexisnexis)
library(devtools)
# install.packages("tidytext")
library(tidytext)
library(broom)
# install.packages("tidyverse")
library(data.table)
library(tidyverse)
library(purrr)
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
View(my_data1)
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
p <- purrr::as_mapper(~identical(., character(0)))
x[p(x)] <- NA
return(x)
}
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
knitr::opts_chunk$set(echo = TRUE)
summarise(word_list = glue::glue_collapse(query_hits, sep = ";")
my_data1_grouped <- my_data1 %>%
knitr::opts_chunk$set(echo = TRUE)
my_data1grouped <- my_data1 %>%
group_by(document) %>%
summarise(word_list = glue::glue_collapse(query_hits, sep = ";")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
head(my_data1grouped)
my_data1grouped <- my_data1 %>%
group_by(document) %>%
summarise(word_list = glue::glue_collapse(query_hits, sep = ";")
head(my_data1grouped)
knitr::opts_chunk$set(echo = TRUE)
head(my_data1grouped, 10)
knitr::opts_chunk$set(echo = TRUE)
View(my_data1grouped)
knitr::opts_chunk$set(echo = TRUE)
my_data1grouped <- my_data1 %>%
group_by(document) %>%
summarise(word_list = glue::glue_collapse(query_hits, sep = ";"))
library(stringr)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
p <- purrr::as_mapper(~identical(., character(0)))
x[p(x)] <- NA
return(x)
}
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
my_data1grouped <- my_data1 %>%
group_by(document) %>%
summarise(word_list = glue::glue_collapse(query_hits, sep = ";"))
View(my_data1grouped)
knitr::opts_chunk$set(echo = TRUE)
my_data1grouped <- my_data1 %>%
group_by(document)
knitr::opts_chunk$set(echo = TRUE)
my_data1grouped <- my_data1 %>%
group_by(document, query_hits)%>%
summarise(word_list = glue::glue_collapse(query_hits, sep = ";"))
View(my_data1grouped)
knitr::opts_chunk$set(echo = TRUE)
my_data1grouped <- my_data1 %>%
group_by(document, query_hits)
View(my_data1grouped)
knitr::opts_chunk$set(echo = TRUE)
# install.packages("pdftools", "devtools")
library(pdftools)
library(tm)
# library(tm.plugin.lexisnexis)
library(devtools)
# install.packages("tidytext")
library(tidytext)
library(broom)
# install.packages("tidyverse")
library(data.table)
library(tidyverse)
library(purrr)
directory <- "G:/TextAnalysis/lexisUni/sample_pdfs"
pdfs_2 <- paste(directory, "/", list.files(directory, pattern = "*.pdf", ignore.case = T), sep = "")
pdfs_names <- list.files(directory, pattern = "*.pdf", ignore.case = T)
pdfs_names
pdfs_text <- map(pdfs_2, pdftools::pdf_text)
head(pdfs_text,2)
my_data <- data_frame(document = pdfs_names, text = pdfs_text)
View(my_data)
# dataset with each page in one row
my_data1 <- my_data %>%
unnest() # splits pdf text by page and removes list format ( c("")) since each element of the list is now its own row.
View(my_data1)
#Dataset with each work in or row associated with its pdf source
my_data2 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = word, input = text, token =
"words", to_lower = T
# strip_numeric = TRUE
)%>%
filter(!word %in% c("lexis",
"nexis", "Uni",
"about lexisnexis",
"Privacy Policy",
"Terms & Conditions", "Copyright Â© 2018 LexisNexis",
" | ",  "@", "lexisnexis"))
# %>% gsub("[^A-Za-z0-9,;._-]","")
View(my_data2)
negative_words <- paste0(c('negative|postpone|against|delay|lawsuit|litigation|protest|^cost|^stop'))
# Function to replace `character(0)` with NAs as NULL values are dropped when flattening list
# inspired by: https://colinfay.me/purrr-set-na/
charnull_set <- function(x){
p <- purrr::as_mapper(~identical(., character(0)))
x[p(x)] <- NA
return(x)
}
my_data1 <- my_data1 %>%
mutate(query_hits = str_extract_all(text, pattern = regex(negative_words, ignore_case=TRUE)) %>%  # Extract all the keywords
map(~charnull_set(.x)) %>%   # Replace character(0) with NAs
map_chr(~glue::glue_collapse(.x, sep = ";")) %>%   # collapse the multiple hits
tolower) # all our keywords are lower case
View(my_data1)
# %>% gsub("[^A-Za-z0-9,;._-]","")
View(my_data2)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T
# strip_numeric = TRUE
)
View(my_data4)
my_data3 <- my_data2 %>%
group_by(document, word) %>%
summarise(count = n())
my_data3_bind <-my_data3 %>%
left_join(get_sentiments("afinn"), by = "word")
View(my_data3_bind)
knitr::opts_chunk$set(echo = TRUE)
total_sentiment <- my_data3_bind %>%
filter(score !="NA") %>%
summarise(totals = mean(score)) %>%
group_by()
View(total_sentiment)
View(get_sentiments("afinn"))
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = ngrams, input = text, token = "paragraph", n = 5, to_lower = T
# strip_numeric = TRUE
)
?unnest_tokens()
knitr::opts_chunk$set(echo = TRUE)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = ngrams, input = text, token = "parapgraphs", n = 5, to_lower = T
# strip_numeric = TRUE
)
knitr::opts_chunk$set(echo = TRUE)
### Packages
# install.packages("pdftools", "devtools")
library(pdftools)
library(tm)
# library(tm.plugin.lexisnexis)
library(devtools)
# install.packages("tidytext")
library(tidytext)
library(broom)
# install.packages("tidyverse")
library(data.table)
library(tidyverse)
library(purrr)
### Read in
Windbelt_Project_Database_1_ <- read_csv("G:/Data/wind project dataset/WindbeltProjectDatabase.csv")
wind_df <- Windbelt_Project_Database_1_
#each row is a pdf doc name (document) with the full pdf text
my_data <- data_frame(document = pdfs_names, text = pdfs_text)
View(my_data)
# text column : each row is an element of a list
# dataset with each page in one row
my_data1 <- my_data %>%
unnest() # splits pdf text by page and removes list format ( c("")) since each element of the list is now its own row.
View(my_data1)
#Dataset with each work in or row associated with its pdf source
my_data2 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = word, input = text, token =
"words", to_lower = T
# strip_numeric = TRUE
)%>%
filter(!word %in% c("lexis",
"nexis", "Uni",
"about lexisnexis",
"Privacy Policy",
"Terms & Conditions", "Copyright Â© 2018 LexisNexis",
" | ",  "@", "lexisnexis"))
# %>% gsub("[^A-Za-z0-9,;._-]","")
View(my_data2)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T)
View(my_data4)
# note: unnest_tokens() splits text by respective element (ie word, phrase, ...) word is default
directory <- "G:/TextAnalysis/lexisUni/sample_pdfs"
pdfs_2 <- paste(directory, "/", list.files(directory, pattern = "*.pdf", ignore.case = T), sep = "")
pdfs_names <- list.files(directory, pattern = "*.pdf", ignore.case = T)
pdfs_names
pdfs_text <- map(pdfs_2, pdftools::pdf_text)
head(pdfs_text,2)
knitr::opts_chunk$set(echo = TRUE)
#each row is a pdf doc name (document) with the full pdf text
my_data <- data_frame(document = pdfs_names, text = pdfs_text)
View(my_data)
# text column : each row is an element of a list
# dataset with each page in one row
my_data1 <- my_data %>%
unnest() # splits pdf text by page and removes list format ( c("")) since each element of the list is now its own row.
View(my_data1)
knitr::opts_chunk$set(echo = TRUE)
#Dataset with each work in or row associated with its pdf source
my_data2 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = word, input = text, token =
"words", to_lower = T
# strip_numeric = TRUE
)%>%
filter(!word %in% c("lexis",
"nexis", "Uni",
"about lexisnexis",
"Privacy Policy",
"Terms & Conditions", "Copyright Â© 2018 LexisNexis",
" | ",  "@", "lexisnexis"))
# %>% gsub("[^A-Za-z0-9,;._-]","")
View(my_data2)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T)
View(my_data4)
# note: unnest_tokens() splits text by respective element (ie word, phrase, ...) word is default
#Dataset with each work in or row associated with its pdf source
my_data2 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = word, input = text, token =
"words", to_lower = T
# strip_numeric = TRUE
)%>%
filter(!word %in% c("lexis",
"nexis", "Uni",
"about lexisnexis",
"Privacy Policy",
"Terms & Conditions", "Copyright Â© 2018 LexisNexis",
" | ",  "@", "lexisnexis"))
# %>% gsub("[^A-Za-z0-9,;._-]","")
View(my_data2)
my_data4 <- my_data %>%
unnest() %>%
tidytext::unnest_tokens(output = ngrams, input = text, token = "ngrams", n = 5, to_lower = T)
View(my_data4)
# note: unnest_tokens() splits text by respective element (ie word, phrase, ...) word is default
knitr::opts_chunk$set(echo = TRUE)
my_data3 <- my_data4 %>%
group_by(document, ngrams) %>%
summarise(count = n())
View(my_data3)
knitr::opts_chunk$set(echo = TRUE)
count_mydata3_bind <-my_data3_bind %>%
count(word, score, sort = TRUE)
View(count_mydata3_bind)
